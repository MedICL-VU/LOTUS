{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f30909fb1ea2d7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7989a428dcfb0dc9",
   "metadata": {},
   "source": [
    "To analyze the impact of shrinkage levels on reconstruction performance by shrinking the binary mask, here we provide 2 methods:\n",
    "\n",
    "- 1. Following the reviewer's suggestion, shrinking the mask by one pixel, you can find the code in the 4th block\n",
    "- 2. Shrinking the mask by self-defined downsampling rate. You can find the code in the val_transforms part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import first, set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import L1Loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.inferers import LatentDiffusionInferer,DiffusionInferer\n",
    "from generative.losses import PatchAdversarialLoss, PerceptualLoss\n",
    "from generative.networks.nets import AutoencoderKL, DiffusionModelUNet, PatchDiscriminator\n",
    "from generative.networks.schedulers import DDPMScheduler, DDIMScheduler\n",
    "\n",
    "from mytransforms import *\n",
    "from utiles import *\n",
    "from generative.networks.nets import ControlNet\n",
    "from generative.inferers import ControlNetDiffusionInferer,ControlNetLatentDiffusionInferer\n",
    "print_config()\n",
    "\n",
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71a1af3f6d43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dim = 64\n",
    "batch_size = 4\n",
    "img_space = 4\n",
    "train_name = 'your_model_name'\n",
    "start_time = time.time()\n",
    "\n",
    "input_data_dir = '/your_data_path'\n",
    "save_dir = '/results/Repaint3D/'+train_name\n",
    "save_vis_dir = os.path.join(save_dir,\"vis\")\n",
    "os.makedirs(save_vis_dir, exist_ok=True)\n",
    "\n",
    "img_list_pattern = list_nii_files(input_data_dir)\n",
    "img_list = natsorted(img_list_pattern)\n",
    "\n",
    "\n",
    "val_img_list = img_list\n",
    "\n",
    "val_files = [\n",
    "    {\n",
    "        \"image\":val_img_list[i],\n",
    "    }\n",
    "    for i in range(len(val_img_list))\n",
    "]\n",
    "\n",
    "\n",
    "channel = 0  # 0 = Flair\n",
    "assert channel in [0, 1, 2, 3], \"Choose a valid channel\"\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.Lambdad(keys=\"image\", func=lambda x: x[channel, :, :, :]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "        transforms.EnsureTyped(keys=[\"image\"]),\n",
    "        transforms.Orientationd(keys=[\"image\"], axcodes=\"LPI\"), # TODO Here change RAS to LPI\n",
    "        Copyd(keys=[\"image\"], new_key=[\"ori_img\"]),\n",
    "        transforms.Spacingd(keys=[\"image\"], pixdim=(img_space, img_space, img_space), mode=(\"bilinear\")),\n",
    "        # TODO here may add an affine later\n",
    "        transforms.ForegroundMaskD(keys = [\"image\"], new_key_prefix = \"mask\", threshold = 0.999, invert = True), # so we get a key called\n",
    "        \n",
    "        transforms.CenterSpatialCropd(keys=[\"maskimage\"], roi_size=(img_dim, img_dim, img_dim)),\n",
    "        transforms.CenterSpatialCropd(keys=[\"image\"], roi_size=(img_dim, img_dim, img_dim)),\n",
    "\n",
    "        \n",
    "        # you can shrink the binary mask by a certain downsampling rate by uncommenting the following code\n",
    "        # # ***************************************************************************\n",
    "        # transforms.Spacingd(keys=[\"maskimage\"], pixdim=(1.2, 1.2, 1.2), mode=(\"nearest\")),\n",
    "        # transforms.ForegroundMaskD(keys = [\"maskimage\"], threshold = 0.999, invert = True),\n",
    "        # transforms.SpatialPadd(keys=[\"maskimage\"],spatial_size = (img_dim, img_dim, img_dim)),\n",
    "        # # ***************************************************************************\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "val_ds = CacheDataset(data = val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ab76bc99fb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=[256, 256, 512],\n",
    "    attention_levels=[False, False, True],\n",
    "    num_head_channels=[0, 0, 512],\n",
    "    num_res_blocks=2,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "model_path = 'pretrained_ddpm_model.pth'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "num_step = 1000\n",
    "scheduler = DDPMScheduler(num_train_timesteps=num_step , schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195)\n",
    "# scheduler = DDIMScheduler(num_train_timesteps=num_step, schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195)\n",
    "inferer = DiffusionInferer(scheduler)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-5)\n",
    "\n",
    "scaler = GradScaler()\n",
    "total_start = time.time()\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef686b87387b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with autocast(enabled=True):\n",
    "        first_val_batch = first(val_loader)\n",
    "        images = first_val_batch[\"image\"].to(device)\n",
    "        masks = first_val_batch[\"maskimage\"].to(device)\n",
    "\n",
    "\n",
    "        # you can shrink the binary mask by 1 pixel by uncommenting the following code\n",
    "        # # ***************************************************************************\n",
    "        # structuring_element = torch.ones((1, 1, 3, 3, 3), dtype=torch.float32, device=\"cuda\")\n",
    "        # padded_masks = F.pad(masks, (1, 1, 1, 1, 1, 1), mode='constant', value=1)\n",
    "        # eroded_mask = F.conv3d(padded_masks, structuring_element, stride=1, padding=1, groups=1)\n",
    "        # eroded_mask = (eroded_mask == structuring_element.sum()).float()\n",
    "        # eroded_mask = eroded_mask[:, :, 1:-1, 1:-1, 1:-1]\n",
    "        # eroded_mask = eroded_mask.to(torch.uint8)\n",
    "        # masks = eroded_mask\n",
    "        # # ***************************************************************************\n",
    "\n",
    "        masked_images = images*masks # TODO the condition for kidney outpainting\n",
    "        \n",
    "        \n",
    "        num_resample_steps = 2\n",
    "        timesteps = torch.Tensor((999,)).to(device).long()\n",
    "        progress_bar = tqdm(scheduler.timesteps)\n",
    "        val_image_inpainted = torch.randn_like(images).to(device)\n",
    "        \n",
    "        for t in progress_bar:\n",
    "            for u in range(num_resample_steps):\n",
    "                # get the known portion at t-1\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(images).to(device)\n",
    "                    timesteps_prev = torch.Tensor((t - 1,)).to(noise.device).long()\n",
    "                    val_image_inpainted_prev_known = scheduler.add_noise(\n",
    "                        original_samples=masked_images, noise=noise, timesteps=timesteps_prev\n",
    "                    )\n",
    "                else:\n",
    "                    val_image_inpainted_prev_known = masked_images\n",
    "\n",
    "                # perform a denoising step to get the unknown portion at t-1\n",
    "                if t > 0:\n",
    "                    timesteps = torch.Tensor((t,)).to(noise.device).long()\n",
    "                    model_output = model(val_image_inpainted, timesteps=timesteps)\n",
    "                    val_image_inpainted_prev_unknown, _ = scheduler.step(model_output, t, val_image_inpainted)\n",
    "\n",
    "                # combine known and unknown using the mask\n",
    "                val_image_inpainted = torch.where(\n",
    "                    masks == 1, val_image_inpainted_prev_known, val_image_inpainted_prev_unknown\n",
    "                )\n",
    "\n",
    "                # perform resampling\n",
    "                if t > 0 and u < (num_resample_steps - 1):\n",
    "                    # sample x_t from x_t-1\n",
    "                    noise = torch.randn_like(images).to(device)\n",
    "                    val_image_inpainted = (\n",
    "                        torch.sqrt(1 - scheduler.betas[t - 1]) * val_image_inpainted\n",
    "                        + torch.sqrt(scheduler.betas[t - 1]) * noise\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7479da80a411b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_results(*tensors, idx):  # TODO: to visualize the midian result\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    t_n = ['I', 'mask','mI', 'op_image']\n",
    "    len_tensor = len(tensors)\n",
    "    for k in range(3):\n",
    "        for i, tensor in enumerate(tensors, start=1):\n",
    "            ax = fig.add_subplot(3, len_tensor, k*len_tensor + i) # show all 3 panels\n",
    "            # Assuming your tensor has the shape (1,1,128,128,128) and it's on CUDA\n",
    "            tensor = tensor.cpu()  # Move the tensor back to CPU for visualization\n",
    "            # print(f\"tensor's shape is {tensor.shape}\")\n",
    "            if k == 0:\n",
    "                tensor_slice = tensor[idx, 0, :, tensor.shape[-1]//2, :].detach().numpy()  # Select the desired slice and convert to numpy\n",
    "            if k == 1:\n",
    "                tensor_slice = tensor[idx, 0, tensor.shape[-1]//2, :, :].detach().numpy()  # Select the desired slice and convert to numpy\n",
    "            if k == 2:\n",
    "                tensor_slice = tensor[idx, 0, :, :, tensor.shape[-1]//2].detach().numpy()  # Select the desired slice and convert to numpy\n",
    "\n",
    "            ax.imshow(tensor_slice, cmap='gray', vmin=0, vmax=1)\n",
    "            ax.set_title(t_n[i - 1])  # Give each tensor image a title\n",
    "            ax.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "for i in range(images.shape[0]):\n",
    "    vis_results(images, masks, masked_images, val_image_inpainted, idx = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74799b90cb472907",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d31016573ec247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T00:49:33.037212Z",
     "start_time": "2024-11-25T00:49:33.033133Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0da79f693edb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
