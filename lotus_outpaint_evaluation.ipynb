{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai import transforms\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import first, set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import L1Loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generative.inferers import LatentDiffusionInferer\n",
    "from generative.losses import PatchAdversarialLoss, PerceptualLoss\n",
    "from generative.networks.nets import AutoencoderKL, DiffusionModelUNet, PatchDiscriminator\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "\n",
    "from mytransforms import *\n",
    "from utiles import *\n",
    "from generative.networks.nets import ControlNet\n",
    "from generative.inferers import ControlNetDiffusionInferer,ControlNetLatentDiffusionInferer\n",
    "print_config()\n",
    "\n",
    "set_determinism(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T01:12:14.430057Z",
     "start_time": "2025-01-23T01:12:14.411485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove the unnecessary codes\n",
    "batch_size = 4\n",
    "img_dim = 64\n",
    "img_space = 4\n",
    "mask_space = 4\n",
    "n_epochs = 2000\n",
    "adv_weight = 0.01\n",
    "perceptual_weight = 0.01\n",
    "kl_weight = 1e-6\n",
    "latent_c = 1\n",
    "dataname = 'your_data_name'\n",
    "\n",
    "lr = 2.5e-4\n",
    "ldm_epoch = 1500\n",
    "train_name = \"lr\"+str(lr) + 'bs'+str(batch_size) + 'ldmepoch'+ str(ldm_epoch)\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "autoencoder_path = '/the_pretrained_autoencoderkl.pth'\n",
    "unet_path ='/the_pretrained_ldm.pth'\n",
    "\n",
    "input_data_dir = '/data/'+dataname\n"
   ],
   "id": "41fbc80b07d9441d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "img_list_pattern = list_nii_files(input_data_dir)\n",
    "img_list = natsorted(img_list_pattern)\n",
    "\n",
    "# train_img_list = img_list[5*2:]\n",
    "val_img_list = img_list\n",
    "\n",
    "val_files = [\n",
    "    {\n",
    "        \"image\":val_img_list[i],\n",
    "    }\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "\n",
    "channel = 0  # 0 = Flair\n",
    "assert channel in [0, 1, 2, 3], \"Choose a valid channel\"\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.Lambdad(keys=\"image\", func=lambda x: x[channel, :, :, :]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "        transforms.EnsureTyped(keys=[\"image\"]),\n",
    "        transforms.Orientationd(keys=[\"image\"], axcodes=\"LPI\"), # TODO Here change RAS to LPI\n",
    "        Copyd(keys=[\"image\"], new_key=[\"ori_img\"]),\n",
    "        transforms.Spacingd(keys=[\"image\"], pixdim=(img_space, img_space, img_space), mode=(\"bilinear\")),\n",
    "        # TODO here may add an affine later\n",
    "        transforms.ForegroundMaskD(keys = [\"image\"], new_key_prefix = \"mask\", threshold = 0.999, invert = True), # so we get a key called\n",
    "        Copyd(keys=[\"maskimage\",\"maskimage\"], new_key=[\"latent_mask\",\"latent_mask2\"]),\n",
    "        transforms.CenterSpatialCropd(keys=[\"maskimage\"], roi_size=(img_dim, img_dim, img_dim)),\n",
    "        transforms.CenterSpatialCropd(keys=[\"image\"], roi_size=(img_dim, img_dim, img_dim)),\n",
    "        transforms.Resized(keys=[\"latent_mask\"],spatial_size=(14,14,14)), # we just shrink for 2 pixels\n",
    "        transforms.ForegroundMaskD(keys = [\"latent_mask\"], threshold = 0.999, invert = True), # so we get a key called\n",
    "        # transforms.CenterSpatialCropd(keys=[\"latent_mask\"], roi_size=(16, 16, 16)),\n",
    "        transforms.SpatialPadD(keys=[\"latent_mask\"], spatial_size=(16, 16, 16)),\n",
    "        \n",
    "        transforms.Resized(keys=[\"latent_mask2\"],spatial_size=(18,18,18)),\n",
    "        transforms.ForegroundMaskD(keys = [\"latent_mask2\"], threshold = 0.999, invert = True), # so we get a key called\n",
    "        transforms.CenterSpatialCropd(keys=[\"latent_mask2\"], roi_size=(16, 16, 16)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_ds = CacheDataset(data = val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=8, persistent_workers=True)"
   ],
   "id": "309e262c730efcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "check_data = first(val_loader)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "autoencoder = AutoencoderKL(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=(32, 64, 64),\n",
    "    latent_channels=latent_c,\n",
    "    num_res_blocks=1,\n",
    "    norm_num_groups=16,\n",
    "    attention_levels=(False, False, True),\n",
    ")\n",
    "autoencoder.to(device)\n",
    "autoencoder.load_state_dict(torch.load(autoencoder_path))\n",
    "autoencoder.eval()\n",
    "\n",
    "unet = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=latent_c,\n",
    "    out_channels=latent_c,\n",
    "    num_res_blocks=1,\n",
    "    num_channels=(32, 64, 64),\n",
    "    attention_levels=(False, True, True),\n",
    "    num_head_channels=(0, 64, 64),\n",
    ")\n",
    "unet.to(device)\n",
    "\n",
    "unet.load_state_dict(torch.load(unet_path))\n",
    "unet.eval()\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0015, beta_end=0.0195)\n",
    "with torch.no_grad():\n",
    "    with autocast(enabled=True):\n",
    "        z = autoencoder.encode_stage_2_inputs(check_data[\"image\"].to(device))\n",
    "\n",
    "print(f\"Scaling factor set to {1/torch.std(z)}\")\n",
    "scale_factor = 1 / torch.std(z)\n",
    "\n",
    "inferer = LatentDiffusionInferer(scheduler, scale_factor=scale_factor)\n",
    "# optimizer_diff = torch.optim.Adam(params=unet.parameters(), lr=1e-4)\n"
   ],
   "id": "98613c5150c42e89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T01:12:32.957109Z",
     "start_time": "2025-01-23T01:12:32.946922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_ssim import *\n",
    "\n",
    "class NCC:\n",
    "    \"\"\"\n",
    "    Local (over window) normalized cross correlation loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, win=None):\n",
    "        self.win = win\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        # compute local sums via convolution\n",
    "        I= y_true\n",
    "        J = y_pred\n",
    "        eps = 1e-10\n",
    "        cross = (I - torch.mean(I)) * (J - torch.mean(J))\n",
    "        I_var = (I - torch.mean(I)) * (I - torch.mean(I))\n",
    "        J_var = (J - torch.mean(J)) * (J - torch.mean(J))\n",
    "\n",
    "        cc = torch.sum(cross) / torch.sum(torch.sqrt(I_var * J_var + eps))\n",
    "\n",
    "        # test = torch.mean(cc)\n",
    "        return cc\n"
   ],
   "id": "473467830fa41e6f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_centered_tensor(size=16, n=8):\n",
    "    if n >= size:\n",
    "        raise ValueError(\"n should be less than the size of the tensor\")\n",
    "    \n",
    "    # 创建一个大小为 16x16x16 的全零张量\n",
    "    tensor = torch.zeros(size, size, size)\n",
    "    \n",
    "    # 计算中心区域的起始和结束索引\n",
    "    start_index = (size - n) // 2\n",
    "    end_index = start_index + n\n",
    "    \n",
    "    # 将中心区域设置为 1\n",
    "    tensor[start_index:end_index, start_index:end_index, start_index:end_index] = 1\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "ssim_list = []\n",
    "ncc_list = []\n",
    "mse_list = []\n",
    "\n",
    "val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), ncols=110)\n",
    "with torch.no_grad():\n",
    "    with autocast(enabled=True):\n",
    "        for step, first_val_batch in val_progress_bar:\n",
    "        # first_val_batch = first(val_loader)\n",
    "            images = first_val_batch[\"image\"].to(device)\n",
    "            ori_masks = first_val_batch[\"maskimage\"].to(device)\n",
    "    \n",
    "            k = 8\n",
    "            latent_mask = create_centered_tensor(16, k-2).to(device) # LOTUS\n",
    "            # latent_mask = create_centered_tensor(16, k).to(device) # LOTUS*\n",
    "            masks = create_centered_tensor(64, k*4).to(device).unsqueeze(0)\n",
    "            masks = masks.repeat(images.shape[0], 1, 1, 1,1)\n",
    "            op_roi = masks\n",
    "            op_target = images*(1 - op_roi)\n",
    "            # masks = 1 - masks\n",
    "            masked_images = images*masks # TODO the condition for kidney outpainting\n",
    "            \n",
    "            # # Note here the image should also be a destroied version:\n",
    "            # images = masked_images\n",
    "            print(f'masked_images shape is {masked_images.shape}') # masked_images shape is torch.Size([4, 1, 64, 64, 64])\n",
    "            masked_images_latent = autoencoder.encode_stage_2_inputs(masked_images)\n",
    "            images_latent = autoencoder.encode_stage_2_inputs(images)\n",
    "            RmI_L = autoencoder.decode_stage_2_outputs(images_latent / scale_factor)*ori_masks\n",
    "\n",
    "            masked_latent = masked_images_latent * latent_mask.to(device) \n",
    "            # masked_latent = m1L\n",
    "            \n",
    "            \n",
    "            num_resample_steps = 2\n",
    "            timesteps = torch.Tensor((999,)).to(device).long()\n",
    "            progress_bar = tqdm(scheduler.timesteps)\n",
    "            val_image_inpainted = torch.randn_like(images_latent).to(device)\n",
    "            \n",
    "            for t in progress_bar:\n",
    "                for u in range(num_resample_steps):\n",
    "                    # get the known portion at t-1\n",
    "                    if t > 0:\n",
    "                        noise = torch.randn_like(images_latent).to(device)\n",
    "                        timesteps_prev = torch.Tensor((t - 1,)).to(noise.device).long()\n",
    "                        val_image_inpainted_prev_known = scheduler.add_noise(\n",
    "                            original_samples=masked_latent, noise=noise, timesteps=timesteps_prev\n",
    "                        )\n",
    "                    else:\n",
    "                        val_image_inpainted_prev_known = masked_latent\n",
    "    \n",
    "                    # perform a denoising step to get the unknown portion at t-1\n",
    "                    if t > 0:\n",
    "                        timesteps = torch.Tensor((t,)).to(noise.device).long()\n",
    "                        model_output = unet(val_image_inpainted, timesteps=timesteps)\n",
    "                        val_image_inpainted_prev_unknown, _ = scheduler.step(model_output, t, val_image_inpainted)\n",
    "    \n",
    "                    # combine known and unknown using the mask\n",
    "                    val_image_inpainted = torch.where(\n",
    "                        latent_mask == 1, val_image_inpainted_prev_known, val_image_inpainted_prev_unknown\n",
    "                    )\n",
    "    \n",
    "                    # perform resampling\n",
    "                    if t > 0 and u < (num_resample_steps - 1):\n",
    "                        # sample x_t from x_t-1\n",
    "                        noise = torch.randn_like(images_latent).to(device)\n",
    "                        val_image_inpainted = (\n",
    "                            torch.sqrt(1 - scheduler.betas[t - 1]) * val_image_inpainted\n",
    "                            + torch.sqrt(scheduler.betas[t - 1]) * noise\n",
    "                        )\n",
    "            rec_image = autoencoder.decode_stage_2_outputs(val_image_inpainted / scale_factor)\n",
    "            # rec_image = rec_image*ori_masks*(1 - op_roi)\n",
    "            rec_image = (rec_image - torch.min(rec_image))/(torch.max(rec_image) - torch.min(rec_image))*ori_masks*(1 - op_roi)\n",
    "\n",
    "\n",
    "            \n",
    "            for n in range(rec_image.shape[0]):\n",
    "                # compare in the whole image level\n",
    "                rec_patch = rec_image[n:n+1, :, :, :, :]\n",
    "                target_patch = op_target[n:n+1, :, :, :, :]\n",
    "                \n",
    "                ssim_volume = ssim3D(rec_patch, target_patch, window_size=9, size_average=False)\n",
    "                ssim_volume = ssim_volume.squeeze(0).squeeze(0).cpu().numpy()\n",
    "                ssim_score = np.average(ssim_volume)\n",
    "                ssim_list.append(ssim_score)\n",
    "                ncc_score = NCC().loss(target_patch, rec_patch)\n",
    "                ncc_list.append(ncc_score.cpu().numpy())\n",
    "                \n",
    "                mse_volume = (target_patch - rec_patch) ** 2\n",
    "                mse_list.append(np.average(mse_volume.cpu().numpy()))\n",
    "                "
   ],
   "id": "aebbe52e70a956cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# k = 8, 8x4 32 whole image, lotus\n",
    "print(ssim_list)\n",
    "print(ncc_list)\n",
    "print(mse_list)\n",
    "\n",
    "print(f'SSIM ave: {np.mean(ssim_list)}, std: {np.std(ssim_list)}')\n",
    "print(f'NCC ave: {np.mean(ncc_list)}, std: {np.std(ncc_list)}')\n",
    "print(f'MSE ave: {np.mean(mse_list)}, std: {np.std(mse_list)}')"
   ],
   "id": "5e735ea2d9d34e5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vis_results(*tensors, idx):  # TODO: to visualize the midian result\n",
    "    # folder = output_path + \"/LogVis\"\n",
    "    # os.makedirs(folder, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    # t_n = ['I', 'I_L', 'mI', 'mI_L', 'Lm1', 'Lm2', 'Lm', 'm1L', 'm2L','mL', 'OPm1L','OPm2L','OPmL', 'ROPm1L','ROPm2L','ROPmL']\n",
    "    t_n = ['I', 'mask','mI', 'op_image']\n",
    "    len_tensor = len(tensors)\n",
    "    for k in range(3):\n",
    "        for i, tensor in enumerate(tensors, start=1):\n",
    "            ax = fig.add_subplot(3, len_tensor, k*len_tensor + i) # show all 3 panels\n",
    "            # Assuming your tensor has the shape (1,1,128,128,128) and it's on CUDA\n",
    "            tensor = tensor.cpu()  # Move the tensor back to CPU for visualization\n",
    "            # print(f\"tensor's shape is {tensor.shape}\")\n",
    "            if k == 0:\n",
    "                tensor_slice = tensor[idx, 0, :, tensor.shape[-1]//2, :].detach().numpy()  # Select the desired slice and convert to numpy\n",
    "            if k == 1:\n",
    "                tensor_slice = tensor[idx, 0, tensor.shape[-1]//2, :, :].detach().numpy()  # Select the desired slice and convert to numpy\n",
    "            if k == 2:\n",
    "                tensor_slice = tensor[idx, 0, :, :, tensor.shape[-1]//2].detach().numpy()  # Select the desired slice and convert to numpy\n",
    "            # rotate_slice = np.rot90(tensor_slice, k=-1)\n",
    "            # ax.imshow(tensor_slice, cmap='gray', vmin=0, vmax=1)\n",
    "            # if (t_n[i - 1] == 'I') or (t_n[i - 1] == 'mI') or (t_n[i - 1] == 'RmI_L') or (t_n[i - 1] == 'Rm1L') or (t_n[i - 1] == 'Rm2L') or (t_n[i - 1] =='RmL') or (t_n[i - 1] == 'ROPmL') or (t_n[i - 1] == 'Lm1') or (t_n[i - 1] == 'op_image'):\n",
    "            #     ax.imshow(tensor_slice, cmap='gray', vmin=0, vmax=1)\n",
    "            # else:\n",
    "            # ax.imshow((tensor_slice - np.min(tensor_slice))/(np.max(tensor_slice) - np.min(tensor_slice)), cmap='gray', vmin=0, vmax=1)\n",
    "            ax.imshow(tensor_slice, cmap='gray', vmin=0, vmax=1)\n",
    "            ax.set_title(t_n[i - 1])  # Give each tensor image a title\n",
    "            ax.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "for i in range(images.shape[0]):\n",
    "    # vis_results(images, masks, op_target, rec_image, idx = i)\n",
    "    vis_results(images, masks, RmI_L, rec_image, idx = i)"
   ],
   "id": "56e5b7fc613866a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d311c37929db84e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
